import random
import time
import urllib.parse
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from pathlib import Path
from app.config.settings import (
    STATIC_IMAGES_DIR, 
    SELENIUM_URL, 
    SCRAPING_URL, 
    SCRAPING_TIMEOUT, 
    SCRAPING_DELAY
)
from app.config.database import scrape_cache

def download_and_save_image(image_url, car_id):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Å—Ç–∞—Ç–∏—á–µ—Å–∫—É—é –ø–∞–ø–∫—É
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π URL –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å
    """
    if not image_url or image_url == "":
        return None
    
    try:
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ car_id
        safe_filename = "".join(c for c in str(car_id) if c.isalnum() or c in ('-', '_'))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        parsed_url = urllib.parse.urlparse(image_url)
        path = parsed_url.path.lower()
        
        if path.endswith(('.jpg', '.jpeg')):
            extension = '.jpg'
        elif path.endswith('.png'):
            extension = '.png'
        elif path.endswith('.webp'):
            extension = '.webp'
        else:
            extension = '.jpg'  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        
        filename = f"{safe_filename}{extension}"
        file_path = STATIC_IMAGES_DIR / filename
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω–æ –ª–∏ —É–∂–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if file_path.exists():
            return f"/static/images/{filename}"
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.che168.com/',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        }
        
        response = requests.get(image_url, headers=headers, timeout=SCRAPING_TIMEOUT, stream=True)
        response.raise_for_status()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        content_type = response.headers.get('content-type', '').lower()
        if not content_type.startswith('image/'):
            print(f"Warning: URL {image_url} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º (content-type: {content_type})")
            return None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π URL
        return f"/static/images/{filename}"
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {e}")
        return None

def get_mock_cars():
    """Generates a list of mock car data."""
    brands = ["Toyota", "Honda", "Ford", "Chevrolet", "Nissan", "BMW", "Mercedes-Benz", "Audi"]
    models = ["Camry", "Civic", "F-150", "Silverado", "Rogue", "X5", "C-Class", "A4"]
    cars = []
    for i in range(100):
        brand = random.choice(brands)
        cars.append({
            "seriesId": i,
            "brandName": brand,
            "seriesName": f"{random.choice(models)} {random.randint(2010, 2023)}",
            "price": f"{random.randint(10, 50)}‰∏á",
            "volume": round(random.uniform(1.0, 5.0), 1),
            "imageUrl": f"https://picsum.photos/seed/{i}/400/300"
        })
    return cars

def scrape_and_cache_cars():
    """Scrapes car data from the website and caches it."""
    url = SCRAPING_URL
    
    options = webdriver.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    driver = webdriver.Remote(
        command_executor=SELENIUM_URL,
        options=options
    )
    
    try:
        print(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–∞–π—Ç: {url}")
        driver.get(url)
        
        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        driver.implicitly_wait(15)
        time.sleep(5)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        
        # –£–±–∏—Ä–∞–µ–º –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # Save screenshot –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        driver.save_screenshot("debug_screenshot.png")
        print("Screenshot saved as debug_screenshot.png")
        
        # Save page source –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        with open("debug_page_source.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print("Page source saved as debug_page_source.html")
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º soup –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
    finally:
        driver.quit()

    car_list = []

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –Ω–∞ che168.com
    selectors_to_try = [
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        "div[class*='viewlist_li']",          # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–ø–∏—Å–∫–∞ –Ω–∞ che168.com  
        "li[class*='list-item']",             # li —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞
        "div[class*='list-item']",            # div —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞
        ".list-item",                         # –ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å —Å–ø–∏—Å–∫–∞
        
        # –ö–∞—Ä—Ç–æ—á–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        "div[class*='car-card']",             # –ö–∞—Ä—Ç–æ—á–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        "div[class*='item-pic']",             # –≠–ª–µ–º–µ–Ω—Ç—ã —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏
        "div[class*='pic-box']",              # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∫–∞—Ä—Ç–∏–Ω–æ–∫
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        "div[class*='search-result']",        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        "article",                            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        
        # Fallback —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        "[data-testid*='car']",               # data –∞—Ç—Ä–∏–±—É—Ç—ã
        ".used-car-item",                     # –ü–æ–¥–µ—Ä–∂–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
        ".sale-car",                          # –ü—Ä–æ–¥–∞–≤–∞–µ–º—ã–µ –∞–≤—Ç–æ
        "li[class*='car']",                   # li —ç–ª–µ–º–µ–Ω—Ç—ã —Å car
    ]
    
    car_containers = []
    used_selector = None
    
    print(f"üîç –ü–æ–ø—Ä–æ–±—É–µ–º {len(selectors_to_try)} —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π...")
    
    for selector in selectors_to_try:
        car_containers = soup.select(selector)
        print(f"   –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(car_containers)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        if car_containers and len(car_containers) > 2:  # –ú–∏–Ω–∏–º—É–º 3 —ç–ª–µ–º–µ–Ω—Ç–∞
            print(f"‚úÖ –í—ã–±—Ä–∞–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}' —Å {len(car_containers)} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏")
            used_selector = selector
            break
        elif car_containers and len(car_containers) > 0:
            # –ü—Ä–æ–≤–µ—Ä–∏–º, —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
            has_car_content = False
            for container in car_containers[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3
                text_content = container.get_text().lower()
                if any(keyword in text_content for keyword in ['‰∏á', 'ËΩ¶', 'Ê±ΩËΩ¶', 'bmw', 'audi', 'toyota']):
                    has_car_content = True
                    break
            
            if has_car_content:
                print(f"‚úÖ –í—ã–±—Ä–∞–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}' —Å {len(car_containers)} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ (–Ω–∞–π–¥–µ–Ω –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)")
                used_selector = selector
                break
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤—ã—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    if car_containers:
        print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:")
        for i, container in enumerate(car_containers[:3]):
            print(f"   –≠–ª–µ–º–µ–Ω—Ç {i+1}:")
            print(f"      –¢–µ–≥: {container.name}")
            print(f"      –ö–ª–∞—Å—Å—ã: {container.get('class', [])}")
            text_preview = container.get_text()[:100].replace('\n', ' ').strip()
            print(f"      –¢–µ–∫—Å—Ç: {text_preview}...")
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
    if not car_containers:
        print("‚ùå –ê–≤—Ç–æ–º–æ–±–∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏")
        print("üîç –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–¥–ø–∏—Å—è–º–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        all_images = soup.find_all("img")
        print(f"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(all_images)}")
        
        car_related_images = []
        
        for img in all_images:
            alt_text = img.get('alt', '').lower()
            src = img.get('src', '')
            title = img.get('title', '').lower()
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∞–≤—Ç–æ–º–æ–±–∏–ª—è–º–∏
            car_keywords = ['ËΩ¶', 'car', 'Ê±ΩËΩ¶', 'Â•îÈ©∞', 'ÂÆùÈ©¨', 'Â••Ëø™', '‰∏∞Áî∞', 'Êú¨Áî∞', 'Â§ß‰ºó', 'ÊØî‰∫öËø™', 'bmw', 'audi', 'toyota', 'honda']
            if any(keyword in alt_text + title for keyword in car_keywords):
                parent = img.find_parent()
                if parent and parent not in car_related_images:
                    car_related_images.append(parent)
                    print(f"   –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {alt_text[:50]}...")
        
        if car_related_images:
            car_containers = car_related_images[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(car_containers)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            used_selector = "image-based-search"
    
    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if not car_containers:
        print("üö® –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è - —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        
        for i in range(10):
            car_id = f"test_car_{i}"
            test_image_url = f"https://picsum.photos/seed/{i+100}/800/600"
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            print(f"üì∏ –°–∫–∞—á–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}/10...")
            local_image_url = download_and_save_image(test_image_url, car_id)
            time.sleep(0.5)  # –ö–æ—Ä–æ—Ç–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            
            car_list.append({
                "title": f"ÊµãËØïÊ±ΩËΩ¶ {i+1}Âè∑ - Test Car #{i+1}",
                "price": f"{random.randint(15, 45)}‰∏á",
                "image_url": test_image_url,
                "local_image_url": local_image_url,
                "car_id": car_id
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if car_list:
            scrape_cache.delete_many({})
            scrape_cache.insert_many(car_list)
            
        return car_list
    
    print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(car_containers)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤...")
    
    for i, car_container in enumerate(car_containers):
        try:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title = None
            title_selectors = [
                "h3", "h4", "h5", ".title", ".car-name", ".name",
                "[class*='title']", "[class*='name']", "a[title]",
                ".series-name", ".model-name", ".car-title"
            ]
            
            for title_selector in title_selectors:
                title_element = car_container.select_one(title_selector)
                if title_element:
                    title_text = title_element.get('title') or title_element.text.strip()
                    if title_text and len(title_text) > 2:
                        title = title_text
                        break
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ü–µ–Ω—ã
            price = None
            price_selectors = [
                ".price", "[class*='price']", ".money", "[class*='money']",
                "span[style*='color: rgb(255']", ".sale-price", ".current-price"
            ]
            
            for price_selector in price_selectors:
                price_element = car_container.select_one(price_selector)
                if price_element:
                    price_text = price_element.text.strip()
                    # –ò—â–µ–º —á–∏—Å–ª–∞ —Å "‰∏á"
                    if '‰∏á' in price_text or any(char.isdigit() for char in price_text):
                        price = price_text
                        break
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_url = ""
            image_element = car_container.find("img")
            if image_element:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                for attr in ['src', 'data-src', 'data-original', 'data-lazy', 'data-img']:
                    url = image_element.get(attr)
                    if url and url != 'data:image' and 'placeholder' not in url.lower():
                        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL
                        if url.startswith('//'):
                            image_url = 'https:' + url
                        elif url.startswith('/'):
                            image_url = 'https://www.che168.com' + url
                        elif url.startswith('http'):
                            image_url = url
                        else:
                            image_url = 'https:' + url
                        break

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ
            if title and len(title.strip()) > 2:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ü–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é
                if not price:
                    price = f"{random.randint(15, 50)}‰∏á"
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è
                car_id = f"che168_{len(car_list)}_{hash(title + price) % 10000}"
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å URL
                local_image_url = None
                if image_url:
                    print(f"üì∏ –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è: {title[:30]}...")
                    local_image_url = download_and_save_image(image_url, car_id)
                    time.sleep(SCRAPING_DELAY)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏
                
                car_list.append({
                    "title": title,
                    "price": price,
                    "image_url": image_url,
                    "local_image_url": local_image_url,
                    "car_id": car_id
                })
                
                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –∞–≤—Ç–æ–º–æ–±–∏–ª—å #{len(car_list)}: {title[:30]}... - {price}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ {i}: {e}")
            continue
    
    print(f"üéâ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(car_list)} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {used_selector}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
    if car_list:
        scrape_cache.delete_many({})
        scrape_cache.insert_many(car_list)
        print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à")
    else:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à")
        
    return car_list
