from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pymongo import MongoClient
import os
import json
from datetime import datetime, timedelta
from typing import Optional, List
import random
import hmac
import hashlib
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
import urllib.parse
import time
from pathlib import Path

app = FastAPI()

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ CORS middleware Ð´Ð»Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° frontend Ðº backend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],  # Frontend URLs
    allow_credentials=True,
    allow_methods=["*"],  # Ð Ð°Ð·Ñ€ÐµÑˆÐ°ÐµÐ¼ Ð²ÑÐµ HTTP Ð¼ÐµÑ‚Ð¾Ð´Ñ‹
    allow_headers=["*"],  # Ð Ð°Ð·Ñ€ÐµÑˆÐ°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸
)

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ð°Ð¿ÐºÑƒ Ð´Ð»Ñ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² ÐµÑÐ»Ð¸ ÐµÑ‘ Ð½ÐµÑ‚
STATIC_DIR = Path("static/images")
STATIC_DIR.mkdir(parents=True, exist_ok=True)

# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
app.mount("/static", StaticFiles(directory="static"), name="static")

# Telegram Bot Token - REPLACE WITH YOURS
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN", "YOUR_TELEGRAM_BOT_TOKEN")

# MongoDB setup
client = MongoClient(os.environ.get("MONGO_URL", "mongodb://mongo:27017/"))
db = client.cars_db
cars_collection = db.cars
scrape_cache = db.scrape_cache


def download_and_save_image(image_url, car_id):
    """
    Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð² ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¿Ð°Ð¿ÐºÑƒ
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ URL Ð¸Ð»Ð¸ None ÐµÑÐ»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ
    """
    if not image_url or image_url == "":
        return None
    
    try:
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð° Ð¸Ð· car_id
        safe_filename = "".join(c for c in str(car_id) if c.isalnum() or c in ('-', '_'))
        
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð°
        parsed_url = urllib.parse.urlparse(image_url)
        path = parsed_url.path.lower()
        
        if path.endswith(('.jpg', '.jpeg')):
            extension = '.jpg'
        elif path.endswith('.png'):
            extension = '.png'
        elif path.endswith('.webp'):
            extension = '.webp'
        else:
            extension = '.jpg'  # Ð”ÐµÑ„Ð¾Ð»Ñ‚Ð½Ð¾Ðµ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ
        
        filename = f"{safe_filename}{extension}"
        file_path = STATIC_DIR / filename
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð½Ðµ ÑÐºÐ°Ñ‡Ð°Ð½Ð¾ Ð»Ð¸ ÑƒÐ¶Ðµ ÑÑ‚Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
        if file_path.exists():
            return f"/static/images/{filename}"
        
        # Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.che168.com/',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        }
        
        response = requests.get(image_url, headers=headers, timeout=30, stream=True)
        response.raise_for_status()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
        content_type = response.headers.get('content-type', '').lower()
        if not content_type.startswith('image/'):
            print(f"Warning: URL {image_url} Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ (content-type: {content_type})")
            return None
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ„Ð°Ð¹Ð»
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾: {filename}")
        
        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ URL
        return f"/static/images/{filename}"
        
    except Exception as e:
        print(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ {image_url}: {e}")
        return None

def get_mock_cars():
    """Generates a list of mock car data."""
    brands = ["Toyota", "Honda", "Ford", "Chevrolet", "Nissan", "BMW", "Mercedes-Benz", "Audi"]
    models = ["Camry", "Civic", "F-150", "Silverado", "Rogue", "X5", "C-Class", "A4"]
    cars = []
    for i in range(100):
        brand = random.choice(brands)
        cars.append({
            "seriesId": i,
            "brandName": brand,
            "seriesName": f"{random.choice(models)} {random.randint(2010, 2023)}",
            "price": f"{random.randint(10, 50)}ä¸‡",
            "volume": round(random.uniform(1.0, 5.0), 1),
            "imageUrl": f"https://picsum.photos/seed/{i}/400/300"
        })
    return cars

def scrape_and_cache_cars():
    """Scrapes car data from the website and caches it."""
    url = "https://www.che168.com/china/list/"
    
    options = webdriver.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    driver = webdriver.Remote(
        command_executor='http://selenium:4444/wd/hub',
        options=options
    )
    
    try:
        print(f"ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼ Ð½Ð° ÑÐ°Ð¹Ñ‚: {url}")
        driver.get(url)
        
        # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
        driver.implicitly_wait(15)
        time.sleep(5)  # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ð°ÑƒÐ·Ð° Ð´Ð»Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        
        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð´ÐµÑ‚ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # Save screenshot Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸
        driver.save_screenshot("debug_screenshot.png")
        print("Screenshot saved as debug_screenshot.png")
        
        # Save page source Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸
        with open("debug_page_source.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print("Page source saved as debug_page_source.html")
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ñ€Ð¾ÐºÑ€ÑƒÑ‚Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ soup Ð¿Ð¾ÑÐ»Ðµ Ð¿Ñ€Ð¾ÐºÑ€ÑƒÑ‚ÐºÐ¸
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
    finally:
        driver.quit()

    car_list = []

    # Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹ Ð½Ð° che168.com
    selectors_to_try = [
        # ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÑÐ¿Ð¸ÑÐºÐ¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹
        "div[class*='viewlist_li']",          # ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÐºÐ»Ð°ÑÑ ÑÐ¿Ð¸ÑÐºÐ° Ð½Ð° che168.com  
        "li[class*='list-item']",             # li ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ ÑÐ¿Ð¸ÑÐºÐ°
        "div[class*='list-item']",            # div ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ ÑÐ¿Ð¸ÑÐºÐ°
        ".list-item",                         # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ ÐºÐ»Ð°ÑÑ ÑÐ¿Ð¸ÑÐºÐ°
        
        # ÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹
        "div[class*='car-card']",             # ÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹
        "div[class*='item-pic']",             # Ð­Ð»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ñ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ°Ð¼Ð¸
        "div[class*='pic-box']",              # ÐšÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ñ‹ ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ð¾Ðº
        
        # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ°
        # "div[class*='result']",               # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ - Ð»Ð¾Ð²Ð¸Ñ‚ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ
        "div[class*='search-result']",        # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ°
        "article",                            # Ð¡ÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹
        
        # Fallback ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹
        "[data-testid*='car']",               # data Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñ‹
        ".used-car-item",                     # ÐŸÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»Ð¸
        ".sale-car",                          # ÐŸÑ€Ð¾Ð´Ð°Ð²Ð°ÐµÐ¼Ñ‹Ðµ Ð°Ð²Ñ‚Ð¾
        "li[class*='car']",                   # li ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ñ car
    ]
    
    car_containers = []
    used_selector = None
    
    print(f"ðŸ” ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ {len(selectors_to_try)} ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹...")
    
    for selector in selectors_to_try:
        car_containers = soup.select(selector)
        print(f"   Ð¡ÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ '{selector}': Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {len(car_containers)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
        
        if car_containers and len(car_containers) > 2:  # ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
            print(f"âœ… Ð’Ñ‹Ð±Ñ€Ð°Ð½ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ '{selector}' Ñ {len(car_containers)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸")
            used_selector = selector
            break
        elif car_containers and len(car_containers) > 0:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ð¼, ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ Ð»Ð¸ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹
            has_car_content = False
            for container in car_containers[:3]:  # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 3
                text_content = container.get_text().lower()
                if any(keyword in text_content for keyword in ['ä¸‡', 'è½¦', 'æ±½è½¦', 'bmw', 'audi', 'toyota']):
                    has_car_content = True
                    break
            
            if has_car_content:
                print(f"âœ… Ð’Ñ‹Ð±Ñ€Ð°Ð½ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€ '{selector}' Ñ {len(car_containers)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ (Ð½Ð°Ð¹Ð´ÐµÐ½ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚)")
                used_selector = selector
                break
    
    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ°: Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    if car_containers:
        print(f"\nðŸ“‹ ÐÐ½Ð°Ð»Ð¸Ð· Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²:")
        for i, container in enumerate(car_containers[:3]):
            print(f"   Ð­Ð»ÐµÐ¼ÐµÐ½Ñ‚ {i+1}:")
            print(f"      Ð¢ÐµÐ³: {container.name}")
            print(f"      ÐšÐ»Ð°ÑÑÑ‹: {container.get('class', [])}")
            text_preview = container.get_text()[:100].replace('\n', ' ').strip()
            print(f"      Ð¢ÐµÐºÑÑ‚: {text_preview}...")
    
    # Ð•ÑÐ»Ð¸ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¼Ð¸ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð°Ð¼Ð¸
    if not car_containers:
        print("âŒ ÐÐ²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¼Ð¸ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð°Ð¼Ð¸")
        print("ðŸ” ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ñ‡ÐµÑ€ÐµÐ· Ð¿Ð¾Ð¸ÑÐº Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹...")
        
        # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´Ð¿Ð¸ÑÑÐ¼Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹
        all_images = soup.find_all("img")
        print(f"   Ð’ÑÐµÐ³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ: {len(all_images)}")
        
        car_related_images = []
        
        for img in all_images:
            alt_text = img.get('alt', '').lower()
            src = img.get('src', '')
            title = img.get('title', '').lower()
            
            # Ð˜Ñ‰ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÑÐ¼Ð¸
            car_keywords = ['è½¦', 'car', 'æ±½è½¦', 'å¥”é©°', 'å®é©¬', 'å¥¥è¿ª', 'ä¸°ç”°', 'æœ¬ç”°', 'å¤§ä¼—', 'æ¯”äºšè¿ª', 'bmw', 'audi', 'toyota', 'honda']
            if any(keyword in alt_text + title for keyword in car_keywords):
                parent = img.find_parent()
                if parent and parent not in car_related_images:
                    car_related_images.append(parent)
                    print(f"   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»Ñ: {alt_text[:50]}...")
        
        if car_related_images:
            car_containers = car_related_images[:20]  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð¾ 20
            print(f"ðŸ” ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(car_containers)} Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ")
            used_selector = "image-based-search"
    
    # Ð•ÑÐ»Ð¸ Ð²ÑÐµ ÐµÑ‰Ðµ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾, ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    if not car_containers:
        print("ðŸš¨ ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ - ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ")
        
        for i in range(10):
            car_id = f"test_car_{i}"
            test_image_url = f"https://picsum.photos/seed/{i+100}/800/600"
            
            # Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
            print(f"ðŸ“¸ Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ {i+1}/10...")
            local_image_url = download_and_save_image(test_image_url, car_id)
            time.sleep(0.5)  # ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ°Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°
            
            car_list.append({
                "title": f"æµ‹è¯•æ±½è½¦ {i+1}å· - Test Car #{i+1}",
                "price": f"{random.randint(15, 45)}ä¸‡",
                "image_url": test_image_url,
                "local_image_url": local_image_url,
                "car_id": car_id
            })
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² ÐºÑÑˆ
        if car_list:
            scrape_cache.delete_many({})
            scrape_cache.insert_many(car_list)
            
        return car_list
    
    print(f"ðŸ”„ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ {len(car_containers)} Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²...")
    
    for i, car_container in enumerate(car_containers):
        try:
            # Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
            title = None
            title_selectors = [
                "h3", "h4", "h5", ".title", ".car-name", ".name",
                "[class*='title']", "[class*='name']", "a[title]",
                ".series-name", ".model-name", ".car-title"
            ]
            
            for title_selector in title_selectors:
                title_element = car_container.select_one(title_selector)
                if title_element:
                    title_text = title_element.get('title') or title_element.text.strip()
                    if title_text and len(title_text) > 2:
                        title = title_text
                        break
            
            # Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ñ†ÐµÐ½Ñ‹
            price = None
            price_selectors = [
                ".price", "[class*='price']", ".money", "[class*='money']",
                "span[style*='color: rgb(255']", ".sale-price", ".current-price"
            ]
            
            for price_selector in price_selectors:
                price_element = car_container.select_one(price_selector)
                if price_element:
                    price_text = price_element.text.strip()
                    # Ð˜Ñ‰ÐµÐ¼ Ñ‡Ð¸ÑÐ»Ð° Ñ "ä¸‡"
                    if 'ä¸‡' in price_text or any(char.isdigit() for char in price_text):
                        price = price_text
                        break
            
            # Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
            image_url = ""
            image_element = car_container.find("img")
            if image_element:
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñ‹ Ð´Ð»Ñ URL Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
                for attr in ['src', 'data-src', 'data-original', 'data-lazy', 'data-img']:
                    url = image_element.get(attr)
                    if url and url != 'data:image' and 'placeholder' not in url.lower():
                        # Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ URL
                        if url.startswith('//'):
                            image_url = 'https:' + url
                        elif url.startswith('/'):
                            image_url = 'https://www.che168.com' + url
                        elif url.startswith('http'):
                            image_url = url
                        else:
                            image_url = 'https:' + url
                        break

            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ
            if title and len(title.strip()) > 2:
                # Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ Ñ†ÐµÐ½Ñ‹, ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½ÑƒÑŽ
                if not price:
                    price = f"{random.randint(15, 50)}ä¸‡"
                
                # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ID Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»Ñ
                car_id = f"che168_{len(car_list)}_{hash(title + price) % 10000}"
                
                # Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ URL
                local_image_url = None
                if image_url:
                    print(f"ðŸ“¸ Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ: {title[:30]}...")
                    local_image_url = download_and_save_image(image_url, car_id)
                    time.sleep(1)  # Ð—Ð°Ð´ÐµÑ€Ð¶ÐºÐ° Ð¼ÐµÐ¶Ð´Ñƒ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸ÑÐ¼Ð¸
                
                car_list.append({
                    "title": title,
                    "price": price,
                    "image_url": image_url,
                    "local_image_url": local_image_url,
                    "car_id": car_id
                })
                
                print(f"âœ… Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÑŒ #{len(car_list)}: {title[:30]}... - {price}")
            
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð° {i}: {e}")
            continue
    
    print(f"ðŸŽ‰ Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(car_list)} Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹ Ñ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð¼: {used_selector}")
    
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² ÐºÑÑˆ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    if car_list:
        scrape_cache.delete_many({})
        scrape_cache.insert_many(car_list)
        print(f"ðŸ’¾ Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² ÐºÑÑˆ")
    else:
        print("âš ï¸ ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð² ÐºÑÑˆ")
        
    return car_list


def verify_telegram_auth(auth_data):
    if not auth_data:
        return False

    received_hash = auth_data.pop('hash')
    auth_data_list = [f"{key}={value}" for key, value in sorted(auth_data.items())]
    data_check_string = "\n".join(auth_data_list)

    secret_key = hashlib.sha256(TELEGRAM_BOT_TOKEN.encode()).digest()
    calculated_hash = hmac.new(secret_key, data_check_string.encode(), hashlib.sha256).hexdigest()

    return received_hash == calculated_hash

@app.post("/api/auth/telegram")
def auth_telegram(auth_data: dict):
    if not verify_telegram_auth(auth_data.copy()):
        raise HTTPException(status_code=403, detail="Invalid authentication data")

    # Here you would typically create a session or a JWT for the user
    # For simplicity, we'll just return the user data
    return auth_data

@app.get("/api/scrape-cars")
def get_scraped_cars():
    """Returns scraped car data, using cache if available."""
    cached_cars = list(scrape_cache.find({}, {"_id": 0}))
    if cached_cars:
        return {
            "source": "cache",
            "count": len(cached_cars),
            "data": json.loads(json.dumps(cached_cars, default=str))
        }

    car_list = scrape_and_cache_cars()
    return {
        "source": "live",
        "count": len(car_list),
        "data": car_list
    }

@app.post("/api/refresh-cache")
def refresh_cache():
    """Force refresh the car cache by scraping new data."""
    try:
        car_list = scrape_and_cache_cars()
        return {
            "success": True,
            "message": f"Cache refreshed with {len(car_list)} cars",
            "count": len(car_list)
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"Failed to refresh cache: {str(e)}"
        }

@app.get("/api/images/stats")
def get_images_stats():
    """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ ÑÐºÐ°Ñ‡Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹."""
    try:
        images_dir = STATIC_DIR
        if not images_dir.exists():
            return {"total_images": 0, "total_size": 0, "status": "directory_not_found"}
        
        image_files = list(images_dir.glob("*"))
        total_size = sum(f.stat().st_size for f in image_files if f.is_file())
        
        return {
            "total_images": len(image_files),
            "total_size": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "directory": str(images_dir),
            "status": "ok"
        }
    except Exception as e:
        return {"error": str(e), "status": "error"}

@app.post("/api/images/cleanup")
def cleanup_images():
    """ÐžÑ‡Ð¸Ñ‰Ð°ÐµÑ‚ Ð¿Ð°Ð¿ÐºÑƒ Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸."""
    try:
        images_dir = STATIC_DIR
        if not images_dir.exists():
            return {"message": "Directory not found", "status": "ok"}
        
        deleted_count = 0
        for image_file in images_dir.glob("*"):
            if image_file.is_file():
                image_file.unlink()
                deleted_count += 1
        
        return {
            "message": f"Deleted {deleted_count} images",
            "deleted_count": deleted_count,
            "status": "ok"
        }
    except Exception as e:
        return {"error": str(e), "status": "error"}


@app.get("/")
def read_root():
    return {"Hello": "World"}

@app.get("/api/cars")
def get_cars(
    page: int = 1,
    page_size: int = 10,
    sort_by: Optional[str] = None,
    sort_order: str = "asc",
    title: Optional[str] = None,
    price_from: Optional[str] = None,
    price_to: Optional[str] = None,
):
    # Get cached scraped data
    cached_cars = list(scrape_cache.find({}, {"_id": 0}))
    
    # If no cached data, scrape fresh data
    if not cached_cars:
        car_list = scrape_and_cache_cars()
        cached_cars = list(scrape_cache.find({}, {"_id": 0}))
    
    # Apply filters
    filtered_cars = cached_cars
    
    if title:
        filtered_cars = [car for car in filtered_cars if title.lower() in car.get("title", "").lower()]
    
    if price_from:
        # Extract numeric price for comparison
        filtered_cars = [car for car in filtered_cars if float(car.get("price", "0").replace("ä¸‡", "")) >= float(price_from)]
    
    if price_to:
        # Extract numeric price for comparison
        filtered_cars = [car for car in filtered_cars if float(car.get("price", "0").replace("ä¸‡", "")) <= float(price_to)]

    # Apply sorting
    if sort_by:
        reverse = sort_order == "desc"
        if sort_by == "title":
            filtered_cars.sort(key=lambda x: x.get("title", ""), reverse=reverse)
        elif sort_by == "price":
            filtered_cars.sort(key=lambda x: float(x.get("price", "0").replace("ä¸‡", "")), reverse=reverse)

    total_cars = len(filtered_cars)
    
    # Apply pagination
    start_index = (page - 1) * page_size
    end_index = start_index + page_size
    paginated_cars = filtered_cars[start_index:end_index]

    return {
        "total": total_cars,
        "page": page,
        "page_size": page_size,
        "data": json.loads(json.dumps(paginated_cars, default=str))
    } 

@app.get("/api/health")
def get_health():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ ÑÑ‚Ð°Ñ‚ÑƒÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð¸ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ‹Ñ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²."""
    health_status = {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "services": {}
    }
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° MongoDB
    try:
        client.admin.command('ping')
        health_status["services"]["mongodb"] = "ok"
    except Exception as e:
        health_status["services"]["mongodb"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Selenium
    try:
        # ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚Ð¸ Selenium Grid
        selenium_url = "http://selenium:4444/wd/hub/status"
        response = requests.get(selenium_url, timeout=5)
        if response.status_code == 200:
            health_status["services"]["selenium"] = "ok"
        else:
            health_status["services"]["selenium"] = f"error: status {response.status_code}"
            health_status["status"] = "degraded"
    except Exception as e:
        health_status["services"]["selenium"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÑÑˆÐ° Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»ÐµÐ¹
    try:
        car_count = scrape_cache.count_documents({})
        health_status["services"]["car_cache"] = f"ok ({car_count} cars)"
    except Exception as e:
        health_status["services"]["car_cache"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    return health_status 

@app.get("/api/debug/page-source")
def get_debug_page_source():
    """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ð¹ HTML Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸."""
    try:
        if os.path.exists("debug_page_source.html"):
            with open("debug_page_source.html", "r", encoding="utf-8") as f:
                content = f.read()
            return {
                "status": "ok",
                "content": content[:50000],  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð¾ 50KB
                "full_length": len(content),
                "message": "HTML source from last scraping attempt"
            }
        else:
            return {
                "status": "not_found",
                "message": "No debug page source found. Run scraping first."
            }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

@app.get("/api/debug/selectors-test")
def test_selectors():
    """Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð½Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ."""
    try:
        if not os.path.exists("debug_page_source.html"):
            return {
                "status": "not_found",
                "message": "No debug page source found. Run scraping first."
            }
        
        with open("debug_page_source.html", "r", encoding="utf-8") as f:
            content = f.read()
        
        soup = BeautifulSoup(content, "html.parser")
        
        # Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹
        selectors_to_test = [
            "div[class*='viewlist_li']",
            "li[class*='list-item']", 
            "div[class*='list-item']",
            "div[class*='car-card']",
            "div[class*='item-pic']",
            "div[class*='pic-box']",
            "div[class*='result']",
            ".list-item",
            "article"
        ]
        
        results = {}
        
        for selector in selectors_to_test:
            elements = soup.select(selector)
            results[selector] = {
                "count": len(elements),
                "sample_classes": [elem.get('class', []) for elem in elements[:3]],
                "sample_text": [elem.get_text()[:100].strip() for elem in elements[:3]]
            }
        
        # Ð¢Ð°ÐºÐ¶Ðµ Ð¸Ñ‰ÐµÐ¼ Ð²ÑÐµ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÐ»Ð°ÑÑÑ‹
        all_divs = soup.find_all(['div', 'li', 'article'])
        unique_classes = set()
        for div in all_divs:
            classes = div.get('class', [])
            for cls in classes:
                if 'list' in cls.lower() or 'item' in cls.lower() or 'car' in cls.lower():
                    unique_classes.add(cls)
        
        return {
            "status": "ok",
            "selector_results": results,
            "interesting_classes": sorted(list(unique_classes)),
            "total_divs": len(all_divs)
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
    } 