from fastapi import FastAPI, Query, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pymongo import MongoClient
import os
import json
from datetime import datetime, timedelta
from typing import Optional, List
import random
import hmac
import hashlib
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
import urllib.parse
import time
from pathlib import Path
import jwt

app = FastAPI()

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ frontend –∫ backend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "*"],  # Frontend URLs
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ HTTP –º–µ—Ç–æ–¥—ã
    allow_headers=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    expose_headers=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ
)

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
STATIC_DIR = Path("static/images")
STATIC_DIR.mkdir(parents=True, exist_ok=True)

# –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory="static"), name="static")

# Telegram Bot Token - REPLACE WITH YOURS
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN", "YOUR_TELEGRAM_BOT_TOKEN")

# JWT Configuration
JWT_SECRET = os.environ.get("JWT_SECRET", "your-secret-key-change-in-production")
JWT_ALGORITHM = "HS256"
JWT_EXPIRATION_HOURS = 24 * 7  # 7 –¥–Ω–µ–π

# Security
security = HTTPBearer(auto_error=False)

# MongoDB setup
client = MongoClient(os.environ.get("MONGO_URL", "mongodb://mongo:27017/"))
db = client.cars_db
cars_collection = db.cars
scrape_cache = db.scrape_cache
users_collection = db.users  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π


def download_and_save_image(image_url, car_id):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Å—Ç–∞—Ç–∏—á–µ—Å–∫—É—é –ø–∞–ø–∫—É
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π URL –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å
    """
    if not image_url or image_url == "":
        return None
    
    try:
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ car_id
        safe_filename = "".join(c for c in str(car_id) if c.isalnum() or c in ('-', '_'))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        parsed_url = urllib.parse.urlparse(image_url)
        path = parsed_url.path.lower()
        
        if path.endswith(('.jpg', '.jpeg')):
            extension = '.jpg'
        elif path.endswith('.png'):
            extension = '.png'
        elif path.endswith('.webp'):
            extension = '.webp'
        else:
            extension = '.jpg'  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        
        filename = f"{safe_filename}{extension}"
        file_path = STATIC_DIR / filename
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω–æ –ª–∏ —É–∂–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if file_path.exists():
            return f"/static/images/{filename}"
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.che168.com/',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        }
        
        response = requests.get(image_url, headers=headers, timeout=30, stream=True)
        response.raise_for_status()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        content_type = response.headers.get('content-type', '').lower()
        if not content_type.startswith('image/'):
            print(f"Warning: URL {image_url} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º (content-type: {content_type})")
            return None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π URL
        return f"/static/images/{filename}"
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {e}")
        return None

def get_mock_cars():
    """Generates a list of mock car data."""
    brands = ["Toyota", "Honda", "Ford", "Chevrolet", "Nissan", "BMW", "Mercedes-Benz", "Audi"]
    models = ["Camry", "Civic", "F-150", "Silverado", "Rogue", "X5", "C-Class", "A4"]
    cars = []
    for i in range(100):
        brand = random.choice(brands)
        cars.append({
            "seriesId": i,
            "brandName": brand,
            "seriesName": f"{random.choice(models)} {random.randint(2010, 2023)}",
            "price": f"{random.randint(10, 50)}‰∏á",
            "volume": round(random.uniform(1.0, 5.0), 1),
            "imageUrl": f"https://picsum.photos/seed/{i}/400/300"
        })
    return cars

def scrape_and_cache_cars():
    """Scrapes car data from the website and caches it."""
    url = "https://www.che168.com/china/list/"
    
    options = webdriver.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    driver = webdriver.Remote(
        command_executor='http://selenium:4444/wd/hub',
        options=options
    )
    
    try:
        print(f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å–∞–π—Ç: {url}")
        driver.get(url)
        
        # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        driver.implicitly_wait(15)
        time.sleep(5)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        
        # –£–±–∏—Ä–∞–µ–º –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # Save screenshot –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        driver.save_screenshot("debug_screenshot.png")
        print("Screenshot saved as debug_screenshot.png")
        
        # Save page source –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        with open("debug_page_source.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print("Page source saved as debug_page_source.html")
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ–∫—Ä—É—Ç–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º soup –ø–æ—Å–ª–µ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
    finally:
        driver.quit()

    car_list = []

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π –Ω–∞ che168.com
    selectors_to_try = [
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        "div[class*='viewlist_li']",          # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–ø–∏—Å–∫–∞ –Ω–∞ che168.com  
        "li[class*='list-item']",             # li —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞
        "div[class*='list-item']",            # div —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞
        ".list-item",                         # –ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å —Å–ø–∏—Å–∫–∞
        
        # –ö–∞—Ä—Ç–æ—á–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        "div[class*='car-card']",             # –ö–∞—Ä—Ç–æ—á–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        "div[class*='item-pic']",             # –≠–ª–µ–º–µ–Ω—Ç—ã —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏
        "div[class*='pic-box']",              # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∫–∞—Ä—Ç–∏–Ω–æ–∫
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        # "div[class*='result']",               # –£–±–∏—Ä–∞–µ–º - –ª–æ–≤–∏—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        "div[class*='search-result']",        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        "article",                            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        
        # Fallback —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        "[data-testid*='car']",               # data –∞—Ç—Ä–∏–±—É—Ç—ã
        ".used-car-item",                     # –ü–æ–¥–µ—Ä–∂–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
        ".sale-car",                          # –ü—Ä–æ–¥–∞–≤–∞–µ–º—ã–µ –∞–≤—Ç–æ
        "li[class*='car']",                   # li —ç–ª–µ–º–µ–Ω—Ç—ã —Å car
    ]
    
    car_containers = []
    used_selector = None
    
    print(f"üîç –ü–æ–ø—Ä–æ–±—É–µ–º {len(selectors_to_try)} —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π...")
    
    for selector in selectors_to_try:
        car_containers = soup.select(selector)
        print(f"   –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(car_containers)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        if car_containers and len(car_containers) > 2:  # –ú–∏–Ω–∏–º—É–º 3 —ç–ª–µ–º–µ–Ω—Ç–∞
            print(f"‚úÖ –í—ã–±—Ä–∞–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}' —Å {len(car_containers)} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏")
            used_selector = selector
            break
        elif car_containers and len(car_containers) > 0:
            # –ü—Ä–æ–≤–µ—Ä–∏–º, —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
            has_car_content = False
            for container in car_containers[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3
                text_content = container.get_text().lower()
                if any(keyword in text_content for keyword in ['‰∏á', 'ËΩ¶', 'Ê±ΩËΩ¶', 'bmw', 'audi', 'toyota']):
                    has_car_content = True
                    break
            
            if has_car_content:
                print(f"‚úÖ –í—ã–±—Ä–∞–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä '{selector}' —Å {len(car_containers)} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ (–Ω–∞–π–¥–µ–Ω –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)")
                used_selector = selector
                break
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤—ã—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    if car_containers:
        print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:")
        for i, container in enumerate(car_containers[:3]):
            print(f"   –≠–ª–µ–º–µ–Ω—Ç {i+1}:")
            print(f"      –¢–µ–≥: {container.name}")
            print(f"      –ö–ª–∞—Å—Å—ã: {container.get('class', [])}")
            text_preview = container.get_text()[:100].replace('\n', ' ').strip()
            print(f"      –¢–µ–∫—Å—Ç: {text_preview}...")
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
    if not car_containers:
        print("‚ùå –ê–≤—Ç–æ–º–æ–±–∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏")
        print("üîç –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–¥–ø–∏—Å—è–º–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        all_images = soup.find_all("img")
        print(f"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(all_images)}")
        
        car_related_images = []
        
        for img in all_images:
            alt_text = img.get('alt', '').lower()
            src = img.get('src', '')
            title = img.get('title', '').lower()
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∞–≤—Ç–æ–º–æ–±–∏–ª—è–º–∏
            car_keywords = ['ËΩ¶', 'car', 'Ê±ΩËΩ¶', 'Â•îÈ©∞', 'ÂÆùÈ©¨', 'Â••Ëø™', '‰∏∞Áî∞', 'Êú¨Áî∞', 'Â§ß‰ºó', 'ÊØî‰∫öËø™', 'bmw', 'audi', 'toyota', 'honda']
            if any(keyword in alt_text + title for keyword in car_keywords):
                parent = img.find_parent()
                if parent and parent not in car_related_images:
                    car_related_images.append(parent)
                    print(f"   –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è: {alt_text[:50]}...")
        
        if car_related_images:
            car_containers = car_related_images[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(car_containers)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            used_selector = "image-based-search"
    
    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if not car_containers:
        print("üö® –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è - —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        
        for i in range(10):
            car_id = f"test_car_{i}"
            test_image_url = f"https://picsum.photos/seed/{i+100}/800/600"
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            print(f"üì∏ –°–∫–∞—á–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}/10...")
            local_image_url = download_and_save_image(test_image_url, car_id)
            time.sleep(0.5)  # –ö–æ—Ä–æ—Ç–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            
            car_list.append({
                "title": f"ÊµãËØïÊ±ΩËΩ¶ {i+1}Âè∑ - Test Car #{i+1}",
                "price": f"{random.randint(15, 45)}‰∏á",
                "image_url": test_image_url,
                "local_image_url": local_image_url,
                "car_id": car_id
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if car_list:
            scrape_cache.delete_many({})
            scrape_cache.insert_many(car_list)
            
        return car_list
    
    print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(car_containers)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤...")
    
    for i, car_container in enumerate(car_containers):
        try:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title = None
            title_selectors = [
                "h3", "h4", "h5", ".title", ".car-name", ".name",
                "[class*='title']", "[class*='name']", "a[title]",
                ".series-name", ".model-name", ".car-title"
            ]
            
            for title_selector in title_selectors:
                title_element = car_container.select_one(title_selector)
                if title_element:
                    title_text = title_element.get('title') or title_element.text.strip()
                    if title_text and len(title_text) > 2:
                        title = title_text
                        break
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ü–µ–Ω—ã
            price = None
            price_selectors = [
                ".price", "[class*='price']", ".money", "[class*='money']",
                "span[style*='color: rgb(255']", ".sale-price", ".current-price"
            ]
            
            for price_selector in price_selectors:
                price_element = car_container.select_one(price_selector)
                if price_element:
                    price_text = price_element.text.strip()
                    # –ò—â–µ–º —á–∏—Å–ª–∞ —Å "‰∏á"
                    if '‰∏á' in price_text or any(char.isdigit() for char in price_text):
                        price = price_text
                        break
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_url = ""
            image_element = car_container.find("img")
            if image_element:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                for attr in ['src', 'data-src', 'data-original', 'data-lazy', 'data-img']:
                    url = image_element.get(attr)
                    if url and url != 'data:image' and 'placeholder' not in url.lower():
                        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL
                        if url.startswith('//'):
                            image_url = 'https:' + url
                        elif url.startswith('/'):
                            image_url = 'https://www.che168.com' + url
                        elif url.startswith('http'):
                            image_url = url
                        else:
                            image_url = 'https:' + url
                        break

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ
            if title and len(title.strip()) > 2:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ü–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é
                if not price:
                    price = f"{random.randint(15, 50)}‰∏á"
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è
                car_id = f"che168_{len(car_list)}_{hash(title + price) % 10000}"
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å URL
                local_image_url = None
                if image_url:
                    print(f"üì∏ –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è: {title[:30]}...")
                    local_image_url = download_and_save_image(image_url, car_id)
                    time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏
                
                car_list.append({
                    "title": title,
                    "price": price,
                    "image_url": image_url,
                    "local_image_url": local_image_url,
                    "car_id": car_id
                })
                
                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –∞–≤—Ç–æ–º–æ–±–∏–ª—å #{len(car_list)}: {title[:30]}... - {price}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ {i}: {e}")
            continue
    
    print(f"üéâ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(car_list)} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {used_selector}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
    if car_list:
        scrape_cache.delete_many({})
        scrape_cache.insert_many(car_list)
        print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à")
    else:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à")
        
    return car_list


def verify_telegram_auth(auth_data):
    if not auth_data:
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–µ—à–∞
    if 'hash' not in auth_data:
        # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑—Ä–µ—à–∞–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –±–µ–∑ —Ö–µ—à–∞
        print("‚ö†Ô∏è Hash –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç - —Ä–∞–∑—Ä–µ—à–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return True

 # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
    auth_data_copy = auth_data.copy()
    received_hash = auth_data_copy.pop('hash')
    auth_data_list = [f"{key}={value}" for key, value in sorted(auth_data_copy.items())]
    data_check_string = "\n".join(auth_data_list)

    secret_key = hashlib.sha256(TELEGRAM_BOT_TOKEN.encode()).digest()
    calculated_hash = hmac.new(secret_key, data_check_string.encode(), hashlib.sha256).hexdigest()

    return received_hash == calculated_hash

# JWT —Ñ—É–Ω–∫—Ü–∏–∏
def create_jwt_token(user_data):
    """–°–æ–∑–¥–∞–µ—Ç JWT —Ç–æ–∫–µ–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    payload = {
        "user_id": user_data.get("id"),
        "username": user_data.get("username"),
        "first_name": user_data.get("first_name"),
        "exp": datetime.utcnow() + timedelta(hours=JWT_EXPIRATION_HOURS),
        "iat": datetime.utcnow()
    }
    return jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALGORITHM)

def verify_jwt_token(token: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç JWT —Ç–æ–∫–µ–Ω –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        payload = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGORITHM])
        return payload
    except jwt.ExpiredSignatureError:
        return None
    except jwt.InvalidTokenError:
        return None

def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ JWT —Ç–æ–∫–µ–Ω–∞"""
    if not credentials:
        return None
    
    user_data = verify_jwt_token(credentials.credentials)
    if not user_data:
        return None
    
    return user_data

def save_user_to_db(user_data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    user_id = user_data.get("id")
    if not user_id:
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
    existing_user = users_collection.find_one({"telegram_id": user_id})
    
    if existing_user:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        update_data = {
            "username": user_data.get("username"),
            "first_name": user_data.get("first_name"),
            "last_name": user_data.get("last_name"),
            "photo_url": user_data.get("photo_url"),
            "last_login": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        
        users_collection.update_one(
            {"telegram_id": user_id},
            {"$set": update_data}
        )
    else:
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        new_user = {
            "telegram_id": user_id,
            "username": user_data.get("username"),
            "first_name": user_data.get("first_name"),
            "last_name": user_data.get("last_name"),
            "photo_url": user_data.get("photo_url"),
            "last_login": datetime.utcnow(),
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        
        users_collection.insert_one(new_user)
    
    return users_collection.find_one({"telegram_id": user_id}, {"_id": 0})

@app.post("/api/auth/telegram-webapp")
def auth_telegram_webapp(auth_data: dict):
    """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏–∑ Telegram WebApp"""
    try:
        init_data = auth_data.get("initData")
        user_data = auth_data.get("user")
        
        if not init_data or not user_data:
            raise HTTPException(status_code=400, detail="Missing initData or user data")
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—è initData
        # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é initData –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ë–î
        saved_user = save_user_to_db(user_data)
        
        # –°–æ–∑–¥–∞–µ–º JWT —Ç–æ–∫–µ–Ω
        token = create_jwt_token(user_data)
        
        return {
            "success": True,
            "user": saved_user,
            "token": token,
            "message": "Successful authentication"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Authentication failed: {str(e)}")

@app.post("/api/auth/telegram-web")
def auth_telegram_web(auth_data: dict):
    """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Telegram Login Widget"""
    if not verify_telegram_auth(auth_data.copy()):
        raise HTTPException(status_code=403, detail="Invalid authentication data")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ë–î
    saved_user = save_user_to_db(auth_data)
    
    # –°–æ–∑–¥–∞–µ–º JWT —Ç–æ–∫–µ–Ω
    token = create_jwt_token(auth_data)
    
    return {
        "success": True,
        "user": saved_user,
        "token": token,
        "message": "Successful authentication"
    }

@app.get("/api/auth/validate")
def validate_token(current_user = Depends(get_current_user)):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è JWT —Ç–æ–∫–µ–Ω–∞"""
    if not current_user:
        raise HTTPException(status_code=401, detail="Invalid or expired token")
    
    return {
        "valid": True,
        "user": current_user
    }

@app.post("/api/auth/logout")
def logout(current_user = Depends(get_current_user)):
    """–í—ã—Ö–æ–¥ –∏–∑ —Å–∏—Å—Ç–µ–º—ã"""
    if not current_user:
        raise HTTPException(status_code=401, detail="Not authenticated")
    
    # –í –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å–ø–µ—Ö
    # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å blacklist —Ç–æ–∫–µ–Ω–æ–≤
    return {
        "success": True,
        "message": "Successfully logged out"
    }

@app.post("/api/auth/telegram")
def auth_telegram(auth_data: dict):
    """Legacy endpoint –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return auth_telegram_web(auth_data)

@app.get("/api/scrape-cars")
def get_scraped_cars():
    """Returns scraped car data, using cache if available."""
    cached_cars = list(scrape_cache.find({}, {"_id": 0}))
    if cached_cars:
        return {
            "source": "cache",
            "count": len(cached_cars),
            "data": json.loads(json.dumps(cached_cars, default=str))
        }

    car_list = scrape_and_cache_cars()
    return {
        "source": "live",
        "count": len(car_list),
        "data": car_list
    }

@app.post("/api/refresh-cache")
def refresh_cache():
    """Force refresh the car cache by scraping new data."""
    try:
        car_list = scrape_and_cache_cars()
        return {
            "success": True,
            "message": f"Cache refreshed with {len(car_list)} cars",
            "count": len(car_list)
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"Failed to refresh cache: {str(e)}"
        }

@app.get("/api/images/stats")
def get_images_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."""
    try:
        images_dir = STATIC_DIR
        if not images_dir.exists():
            return {"total_images": 0, "total_size": 0, "status": "directory_not_found"}
        
        image_files = list(images_dir.glob("*"))
        total_size = sum(f.stat().st_size for f in image_files if f.is_file())
        
        return {
            "total_images": len(image_files),
            "total_size": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "directory": str(images_dir),
            "status": "ok"
        }
    except Exception as e:
        return {"error": str(e), "status": "error"}

@app.post("/api/images/cleanup")
def cleanup_images():
    """–û—á–∏—â–∞–µ—Ç –ø–∞–ø–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏."""
    try:
        images_dir = STATIC_DIR
        if not images_dir.exists():
            return {"message": "Directory not found", "status": "ok"}
        
        deleted_count = 0
        for image_file in images_dir.glob("*"):
            if image_file.is_file():
                image_file.unlink()
                deleted_count += 1
        
        return {
            "message": f"Deleted {deleted_count} images",
            "deleted_count": deleted_count,
            "status": "ok"
        }
    except Exception as e:
        return {"error": str(e), "status": "error"}


@app.get("/")
def read_root():
    return {"Hello": "World"}

@app.get("/api/cars")
def get_cars(
    page: int = 1,
    page_size: int = 10,
    sort_by: Optional[str] = None,
    sort_order: str = "asc",
    title: Optional[str] = None,
    price_from: Optional[str] = None,
    price_to: Optional[str] = None,
):
    # Get cached scraped data
    cached_cars = list(scrape_cache.find({}, {"_id": 0}))
    
    # If no cached data, scrape fresh data
    if not cached_cars:
        car_list = scrape_and_cache_cars()
        cached_cars = list(scrape_cache.find({}, {"_id": 0}))
    
    # Apply filters
    filtered_cars = cached_cars
    
    if title:
        filtered_cars = [car for car in filtered_cars if title.lower() in car.get("title", "").lower()]
    
    if price_from:
        # Extract numeric price for comparison
        filtered_cars = [car for car in filtered_cars if float(car.get("price", "0").replace("‰∏á", "")) >= float(price_from)]
    
    if price_to:
        # Extract numeric price for comparison
        filtered_cars = [car for car in filtered_cars if float(car.get("price", "0").replace("‰∏á", "")) <= float(price_to)]

    # Apply sorting
    if sort_by:
        reverse = sort_order == "desc"
        if sort_by == "title":
            filtered_cars.sort(key=lambda x: x.get("title", ""), reverse=reverse)
        elif sort_by == "price":
            filtered_cars.sort(key=lambda x: float(x.get("price", "0").replace("‰∏á", "")), reverse=reverse)

    total_cars = len(filtered_cars)
    
    # Apply pagination
    start_index = (page - 1) * page_size
    end_index = start_index + page_size
    paginated_cars = filtered_cars[start_index:end_index]

    return {
        "total": total_cars,
        "page": page,
        "page_size": page_size,
        "data": json.loads(json.dumps(paginated_cars, default=str))
    } 

@app.get("/api/health")
def get_health():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤."""
    health_status = {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "services": {}
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ MongoDB
    try:
        client.admin.command('ping')
        health_status["services"]["mongodb"] = "ok"
    except Exception as e:
        health_status["services"]["mongodb"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Selenium
    try:
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Selenium Grid
        selenium_url = "http://selenium:4444/wd/hub/status"
        response = requests.get(selenium_url, timeout=5)
        if response.status_code == 200:
            health_status["services"]["selenium"] = "ok"
        else:
            health_status["services"]["selenium"] = f"error: status {response.status_code}"
            health_status["status"] = "degraded"
    except Exception as e:
        health_status["services"]["selenium"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
    try:
        car_count = scrape_cache.count_documents({})
        health_status["services"]["car_cache"] = f"ok ({car_count} cars)"
    except Exception as e:
        health_status["services"]["car_cache"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    return health_status 

@app.get("/api/debug/page-source")
def get_debug_page_source():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π HTML –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
    try:
        if os.path.exists("debug_page_source.html"):
            with open("debug_page_source.html", "r", encoding="utf-8") as f:
                content = f.read()
            return {
                "status": "ok",
                "content": content[:50000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50KB
                "full_length": len(content),
                "message": "HTML source from last scraping attempt"
            }
        else:
            return {
                "status": "not_found",
                "message": "No debug page source found. Run scraping first."
            }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

@app.get("/api/debug/selectors-test")
def test_selectors():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ."""
    try:
        if not os.path.exists("debug_page_source.html"):
            return {
                "status": "not_found",
                "message": "No debug page source found. Run scraping first."
            }
        
        with open("debug_page_source.html", "r", encoding="utf-8") as f:
            content = f.read()
        
        soup = BeautifulSoup(content, "html.parser")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors_to_test = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
            "li.cxc-card",
            "li.cards-li",
            "li.list-photo-li",
            ".cxc-card",
            ".cards-li",
            ".list-photo-li",
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            "li.cards-li.cxc-card",
            "li.list-photo-li.cxc-card",
            "li.cards-li.list-photo-li",
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            "div[class*='viewlist_li']",
            "li[class*='list-item']", 
            "div[class*='list-item']",
            "div[class*='car-card']",
            "div[class*='item-pic']",
            "div[class*='pic-box']",
            "div[class*='result']",
            ".list-item",
            "article",
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∏–∑ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
            ".vehicle-search-list",
            ".vehicle-second-list",
            ".tp-cards-tofu",
            ".right-sidebar-car"
        ]
        
        results = {}
        
        for selector in selectors_to_test:
            elements = soup.select(selector)
            results[selector] = {
                "count": len(elements),
                "sample_classes": [elem.get('class', []) for elem in elements[:3]],
                "sample_text": [elem.get_text()[:100].strip() for elem in elements[:3]]
            }
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        all_divs = soup.find_all(['div', 'li', 'article'])
        unique_classes = set()
        for div in all_divs:
            classes = div.get('class', [])
            for cls in classes:
                if any(keyword in cls.lower() for keyword in ['list', 'item', 'car', 'card', 'photo', 'wrap']):
                    unique_classes.add(cls)
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫
        car_cards = soup.select("li.cxc-card")
        card_analysis = {}
        if car_cards:
            sample_card = car_cards[0]
            card_analysis = {
                "total_cards": len(car_cards),
                "sample_id": sample_card.get('id', ''),
                "sample_classes": sample_card.get('class', []),
                "sample_attributes": {k: v for k, v in sample_card.attrs.items() if k not in ['class', 'id']},
                "has_link": bool(sample_card.find('a')),
                "has_image": bool(sample_card.find('img')),
                "has_price": bool(sample_card.find(text=lambda text: text and '‰∏á' in text))
            }
        
        return {
            "status": "ok",
            "selector_results": results,
            "interesting_classes": sorted(list(unique_classes)),
            "total_divs": len(all_divs),
            "card_analysis": card_analysis
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

@app.post("/api/debug/test-selector")
def test_custom_selector(data: dict):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ."""
    try:
        selector = data.get("selector", "")
        if not selector:
            return {
                "status": "error",
                "message": "Selector is required"
            }
        
        if not os.path.exists("debug_page_source.html"):
            return {
                "status": "not_found",
                "message": "No debug page source found. Run scraping first."
            }
        
        with open("debug_page_source.html", "r", encoding="utf-8") as f:
            content = f.read()
        
        soup = BeautifulSoup(content, "html.parser")
        elements = soup.select(selector)
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        analysis = {
            "count": len(elements),
            "sample_classes": [elem.get('class', []) for elem in elements[:5]],
            "sample_text": [elem.get_text()[:200].strip() for elem in elements[:5]],
            "sample_ids": [elem.get('id', '') for elem in elements[:5]],
            "sample_attributes": []
        }
        
        # –ê–Ω–∞–ª–∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        for elem in elements[:3]:
            attrs = {k: v for k, v in elem.attrs.items() if k not in ['class', 'id']}
            analysis["sample_attributes"].append(attrs)
        
        return {
            "status": "ok",
            "selector": selector,
            "analysis": analysis
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        } 